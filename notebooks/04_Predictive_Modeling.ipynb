{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Análise Preditiva Simplificada\n",
    "\n",
    "This notebook demonstrates a simplified machine learning process to predict exam scores, as outlined in the \"Análise Preditiva\" section of the portfolio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we saved the cleaned data, we could load it here instead:\n",
    "# df = pd.read_parquet('cleaned_student_data.parquet')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Model and Preprocessing Details for JavaScript Implementation\n",
    "\n",
    "The following cells extract and display the trained parameters from the Linear Regression model (`lr_pipeline`) and the preprocessing steps. This information can be used as a reference for a JavaScript implementation of the same model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracted Model and Preprocessor Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the trained LinearRegression model and ColumnTransformer preprocessor\n",
    "lr_model = lr_pipeline.named_steps['regressor']\n",
    "preprocessor_from_pipeline = lr_pipeline.named_steps['preprocessor']\n",
    "\n",
    "print(\"LinearRegression model retrieved:\", lr_model)\n",
    "print(\"ColumnTransformer preprocessor retrieved:\", preprocessor_from_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression: Intercept and Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Linear Regression Intercept (Bias):\")\n",
    "print(lr_model.intercept_)\n",
    "\n",
    "print(\"\\nLinear Regression Coefficients (Weights):\")\n",
    "print(lr_model.coef_)\n",
    "\n",
    "print(\"\\nFeature names corresponding to the coefficients (after all preprocessing):\")\n",
    "final_feature_names = preprocessor_from_pipeline.get_feature_names_out()\n",
    "print(final_feature_names)\n",
    "\n",
    "print(\"\\nCoefficients with their corresponding feature names:\")\n",
    "for feature, coef in zip(final_feature_names, lr_model.coef_):\n",
    "    print(f\"{feature}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Intercept** is the value of the target variable when all features are zero (or at their reference levels for categorical features). \n",
    "The **Coefficients** represent the change in the target variable for a one-unit change in the corresponding feature, holding all other features constant. For scaled numerical features, this means a one-unit change in the scaled value. For one-hot encoded categorical features, it's the change relative to the dropped category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Details: Numerical Features (StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_transformer_from_pipeline = preprocessor_from_pipeline.named_transformers_['num']\n",
    "# Original numerical features are stored in the ColumnTransformer's 'transformers_' list\n",
    "original_numerical_features = [t[2] for t in preprocessor_from_pipeline.transformers if t[0] == 'num'][0]\n",
    "\n",
    "print(\"Numerical Features Scaled:\", original_numerical_features)\n",
    "print(\"\\nStandardScaler Mean (for each numerical feature):\")\n",
    "print(numerical_transformer_from_pipeline.mean_)\n",
    "print(\"\\nStandardScaler Scale (Standard Deviation, for each numerical feature):\")\n",
    "print(numerical_transformer_from_pipeline.scale_)\n",
    "\n",
    "print(\"\\nNumerical features with their scaling parameters (mean, std_dev):\")\n",
    "for i, feature_name in enumerate(original_numerical_features):\n",
    "    print(f\"{feature_name}: mean={numerical_transformer_from_pipeline.mean_[i]:.4f}, std_dev={numerical_transformer_from_pipeline.scale_[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each numerical feature, the `mean_` and `scale_` (standard deviation) from the `StandardScaler` are shown. These are used to standardize the features using the formula: `z = (x - mean) / scale`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Details: Categorical Features (OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer_from_pipeline = preprocessor_from_pipeline.named_transformers_['cat']\n",
    "# Original categorical features are stored in the ColumnTransformer's 'transformers_' list\n",
    "original_categorical_features = [t[2] for t in preprocessor_from_pipeline.transformers if t[0] == 'cat'][0]\n",
    "\n",
    "print(\"Categorical Features OneHotEncoded:\", original_categorical_features)\n",
    "\n",
    "print(\"\\nOneHotEncoder Categories (for each original categorical feature):\")\n",
    "for i, feature_name in enumerate(original_categorical_features):\n",
    "    print(f\"Feature '{feature_name}': {categorical_transformer_from_pipeline.categories_[i]}\")\n",
    "\n",
    "print(\"\\nOutput feature names from OneHotEncoder (after dropping 'first'):\")\n",
    "# These are the names of the columns generated by the OneHotEncoder\n",
    "# The order matches the order of coefficients for the categorical part\n",
    "ohe_feature_names = categorical_transformer_from_pipeline.get_feature_names_out(original_categorical_features)\n",
    "print(ohe_feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each categorical feature, `categories_` shows all unique categories found during training. \n",
    "Since `drop='first'` was used in `OneHotEncoder`, the first category for each feature is dropped to avoid multicollinearity. The `get_feature_names_out()` method shows the names of the generated columns, implicitly indicating which categories were kept (and thus, which were dropped)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detailed Example of `drop='first'` for JavaScript Implementation:\n",
    "\n",
    "Understanding how `drop='first'` affects the final features is crucial for the JavaScript implementation. Here's how it works for a couple of example features using the categories extracted from the training data:\n",
    "\n",
    "1.  **`gender`**:\n",
    "    *   Original categories found by `OneHotEncoder` (from `categorical_transformer_from_pipeline.categories_[0]`): `['Female', 'Male', 'Other']` (order matters).\n",
    "    *   `drop='first'` means 'Female' (the first in the list) is dropped as the reference category.\n",
    "    *   The resulting features used in the model are `cat__gender_Male` and `cat__gender_Other`.\n",
    "    *   **If input `gender` is 'Female'**: `cat__gender_Male` = 0, `cat__gender_Other` = 0.\n",
    "    *   **If input `gender` is 'Male'**: `cat__gender_Male` = 1, `cat__gender_Other` = 0.\n",
    "    *   **If input `gender` is 'Other'**: `cat__gender_Male` = 0, `cat__gender_Other` = 1.\n",
    "\n",
    "2.  **`diet_quality`**:\n",
    "    *   Original categories found by `OneHotEncoder` (from `categorical_transformer_from_pipeline.categories_[2]` assuming it's the third categorical feature): `['Fair', 'Good', 'Poor']` (order matters).\n",
    "    *   `drop='first'` means 'Fair' (the first in this list) is dropped.\n",
    "    *   The resulting features are `cat__diet_quality_Good` and `cat__diet_quality_Poor`.\n",
    "    *   **If input `diet_quality` is 'Fair'**: `cat__diet_quality_Good` = 0, `cat__diet_quality_Poor` = 0.\n",
    "    *   **If input `diet_quality` is 'Good'**: `cat__diet_quality_Good` = 1, `cat__diet_quality_Poor` = 0.\n",
    "    *   **If input `diet_quality` is 'Poor'**: `cat__diet_quality_Good` = 0, `cat__diet_quality_Poor` = 1.\n",
    "\n",
    "This logic needs to be replicated precisely in JavaScript: for each categorical input, determine which binary columns are generated and set them to 0 or 1 based on the input value and the dropped category for that feature. The order of categories shown by `categorical_transformer_from_pipeline.categories_` for each feature dictates which category is 'first'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Example: Manual Prediction for a Single Instance\n",
    "\n",
    "This cell demonstrates how to manually calculate a prediction for a single raw input instance using the extracted model parameters. This serves as a Python counterpart to the JavaScript prediction logic, ensuring clarity on the transformation and calculation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Sample Raw Input (same as used for JavaScript simulator testing)\n",
    "sample_input = {\n",
    "    'age': 20,\n",
    "    'study_hours_per_day': 4.0,\n",
    "    'social_media_hours': 2.0,\n",
    "    'netflix_hours': 2.0,\n",
    "    'attendance_percentage': 80,\n",
    "    'sleep_hours': 7.0,\n",
    "    'exercise_frequency': 3,\n",
    "    'mental_health_rating': 5,\n",
    "    'gender': 'Female',\n",
    "    'part_time_job': 'No',\n",
    "    'diet_quality': 'Fair',\n",
    "    'parental_education_level': 'High School',\n",
    "    'internet_quality': 'Average',\n",
    "    'extracurricular_participation': 'No'\n",
    "}\n",
    "\n",
    "print(f\"Sample Raw Input: {sample_input}\\n\")\n",
    "\n",
    "# 2. Use Extracted Model Parameters (ensure these variables are set from previous cells in the notebook execution flow)\n",
    "# These variables are assumed to be populated from earlier cells where they were extracted from the pipeline.\n",
    "intercept = lr_model.intercept_ \n",
    "coefficients_list = lr_model.coef_\n",
    "feature_names_ordered_by_coefs = preprocessor_from_pipeline.get_feature_names_out()\n",
    "coefficients = dict(zip(feature_names_ordered_by_coefs, coefficients_list))\n",
    "\n",
    "num_transformer = preprocessor_from_pipeline.named_transformers_['num']\n",
    "original_num_features_in_order = [t[2] for t in preprocessor_from_pipeline.transformers if t[0] == 'num'][0]\n",
    "numerical_means = dict(zip(original_num_features_in_order, num_transformer.mean_))\n",
    "numerical_std_devs = dict(zip(original_num_features_in_order, num_transformer.scale_))\n",
    "\n",
    "cat_transformer = preprocessor_from_pipeline.named_transformers_['cat']\n",
    "original_cat_features_in_order = [t[2] for t in preprocessor_from_pipeline.transformers if t[0] == 'cat'][0]\n",
    "ohe_categories_map = {}\n",
    "for i, feature_name in enumerate(original_cat_features_in_order):\n",
    "    ohe_categories_map[feature_name] = cat_transformer.categories_[i].tolist()\n",
    "\n",
    "print(\"--- Model Parameters Used (from previously executed cells) ---\")\n",
    "print(f\"Intercept: {intercept}\")\n",
    "print(f\"First 3 Coefficients: {{k: v for k, v in list(coefficients.items())[:3]}}\")\n",
    "print(f\"Numerical Features (used by StandardScaler): {original_num_features_in_order}\")\n",
    "print(f\"Numerical Means (first 3): {{k: v for k, v in list(numerical_means.items())[:3]}}\")\n",
    "print(f\"Numerical Std Devs (first 3): {{k: v for k, v in list(numerical_std_devs.items())[:3]}}\")\n",
    "print(f\"Categorical Features (used by OneHotEncoder): {original_cat_features_in_order}\")\n",
    "print(f\"OHE Categories Map (e.g., gender): {{'gender': ohe_categories_map.get('gender')}}\")\n",
    "print(f\"Final Feature Order for Coefficients (first 5): {list(feature_names_ordered_by_coefs[:5])}... Total: {len(feature_names_ordered_by_coefs)}\\n\")\n",
    "\n",
    "# 3. Manual Preprocessing for the Sample Input\n",
    "processed_feature_values = {} # This will store the final values for each feature name in `feature_names_ordered_by_coefs`\n",
    "\n",
    "# a. Scale Numerical Features\n",
    "print(\"--- Processing Numerical Features ---\")\n",
    "for original_feature_name in original_num_features_in_order:\n",
    "    raw_value = sample_input[original_feature_name]\n",
    "    mean = numerical_means[original_feature_name]\n",
    "    std_dev = numerical_std_devs[original_feature_name]\n",
    "    scaled_value = (raw_value - mean) / std_dev\n",
    "    # The final feature name for numerical features is 'num__' + original_feature_name\n",
    "    final_num_feature_name = f\"num__{original_feature_name}\"\n",
    "    processed_feature_values[final_num_feature_name] = scaled_value\n",
    "    print(f\"Processed {final_num_feature_name}: ({raw_value} - {mean:.2f}) / {std_dev:.2f} = {scaled_value:.6f}\")\n",
    "\n",
    "# b. One-Hot Encode Categorical Features (respecting drop='first')\n",
    "print(\"\\n--- Processing Categorical Features ---\")\n",
    "for original_feature_name in original_cat_features_in_order:\n",
    "    raw_value = sample_input[original_feature_name]\n",
    "    categories_for_this_feature = ohe_categories_map[original_feature_name]\n",
    "    dropped_category = categories_for_this_feature[0] # First category is dropped\n",
    "    \n",
    "    # Iterate through all categories found during training for this feature, *except* the dropped one\n",
    "    for category_from_training in categories_for_this_feature[1:]:\n",
    "        # The final feature name for OHE features is 'cat__' + original_feature_name + '_' + category_value\n",
    "        final_ohe_feature_name = f\"cat__{original_feature_name}_{category_from_training}\"\n",
    "        if raw_value == category_from_training:\n",
    "            processed_feature_values[final_ohe_feature_name] = 1\n",
    "        else:\n",
    "            processed_feature_values[final_ohe_feature_name] = 0\n",
    "        print(f\"Processed {final_ohe_feature_name}: (input '{raw_value}') -> {processed_feature_values[final_ohe_feature_name]}\")\n",
    "    \n",
    "    # If the raw_value was the dropped_category, all corresponding OHE features should be 0.\n",
    "    # This is implicitly handled because only non-dropped categories generate feature names.\n",
    "\n",
    "# 4. Construct Final Feature Vector in Correct Order (as per feature_names_ordered_by_coefs)\n",
    "feature_vector_for_prediction = []\n",
    "print(\"\\n--- Constructing Final Feature Vector (in model's expected order) ---\")\n",
    "for final_feature_name in feature_names_ordered_by_coefs:\n",
    "    # If a feature was generated (e.g. cat__gender_Other) it will be in processed_feature_values.\n",
    "    # If it was a dropped category (e.g. cat__gender_Female would not be a key), its effective value is 0\n",
    "    # because its coefficient is not used / it's the base category.\n",
    "    # The .get(final_feature_name, 0) handles cases where a OHE column might not be explicitly set if it's not active,\n",
    "    # though the logic above should set all relevant OHE columns to 0 or 1.\n",
    "    value_to_append = processed_feature_values.get(final_feature_name)\n",
    "    if value_to_append is None:\n",
    "        # This should ideally not happen if all features in feature_names_ordered_by_coefs are accounted for.\n",
    "        # This could happen if a feature name from get_feature_names_out() was not created by our manual process.\n",
    "        # For safety, we assume 0, but this indicates a potential mismatch if it occurs.\n",
    "        print(f\"Warning: Feature {final_feature_name} not found in manually processed features. Using 0.\")\n",
    "        value_to_append = 0 \n",
    "    feature_vector_for_prediction.append(value_to_append)\n",
    "\n",
    "print(f\"Final Ordered Feature Vector (first 5 elements): {np.array(feature_vector_for_prediction[:5])}\")\n",
    "print(f\"Full feature vector has {len(feature_vector_for_prediction)} elements.\")\n",
    "\n",
    "# 5. Calculate the Dot Product and Add Intercept\n",
    "manual_prediction = intercept\n",
    "for i, final_feature_name in enumerate(feature_names_ordered_by_coefs):\n",
    "    manual_prediction += coefficients[final_feature_name] * feature_vector_for_prediction[i]\n",
    "\n",
    "# 6. Print Results\n",
    "print(f\"\\nRaw Manual Prediction: {manual_prediction}\")\n",
    "\n",
    "# Clamp and round the prediction as in the JavaScript simulator\n",
    "clamped_manual_prediction = max(0, min(100, manual_prediction))\n",
    "rounded_manual_prediction = round(clamped_manual_prediction, 2)\n",
    "print(f\"Clamped (0-100) and Rounded Manual Prediction: {rounded_manual_prediction}\")\n",
    "\n",
    "print(\"\\nThis manual calculation should match the JavaScript simulator's output for the same input, \\nand also the result from the 'Test Plan' step if the same input and parameters were used.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Check: All Preprocessed Feature Names in Order\n",
    "\n",
    "This list shows all feature names in the exact order they are fed into the linear regression model after all preprocessing. This order should match the order of the model's coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All feature names from the preprocessor (in order):\")\n",
    "all_processed_feature_names = preprocessor_from_pipeline.get_feature_names_out()\n",
    "print(all_processed_feature_names)\n",
    "\n",
    "print(\"\\nNumber of coefficients:\", len(lr_model.coef_))\n",
    "print(\"Number of preprocessed features:\", len(all_processed_feature_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Preprocessing\n",
    "\n",
    "Based on EDA and HTML feature importance chart, let's select features. The HTML mentions:\n",
    "`study_hours_per_day`, `attendance_percentage`, `mental_health_rating`, `sleep_hours`, `internet_quality`, `parental_education_level`.\n",
    "We'll use these and a few more common sense ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop student_id if it's still there\n",
    "if 'student_id' in df.columns:\n",
    "    df_model = df.drop('student_id', axis=1).copy()\n",
    "else:\n",
    "    df_model = df.copy()\n",
    "\n",
    "X = df_model.drop('exam_score', axis=1)\n",
    "y = df_model['exam_score']\n",
    "\n",
    "# Identify numerical and categorical features for the model\n",
    "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['category', 'object']).columns.tolist()\n",
    "\n",
    "print(\"Numerical Features:\", numerical_features)\n",
    "print(\"Categorical Features:\", categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipelines for numerical and categorical features\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore', drop='first') # drop='first' to avoid multicollinearity\n",
    "\n",
    "# Create a preprocessor object using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('regressor', LinearRegression())])\n",
    "\n",
    "lr_pipeline.fit(X_train, y_train)\n",
    "y_pred_lr = lr_pipeline.predict(X_test)\n",
    "\n",
    "rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))\n",
    "r2_lr = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(\"Linear Regression Performance:\")\n",
    "print(f\"RMSE: {rmse_lr:.2f}\")\n",
    "print(f\"R²: {r2_lr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                             ('regressor', RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1))])\n",
    "\n",
    "rf_pipeline.fit(X_train, y_train)\n",
    "y_pred_rf = rf_pipeline.predict(X_test)\n",
    "\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Regressor Performance:\")\n",
    "print(f\"RMSE: {rmse_rf:.2f}\")\n",
    "print(f\"R²: {r2_rf:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: The HTML reports RMSE of 8.52 (LR) / 6.95 (RF) and R² of 0.78 (LR) / 0.85 (RF). The actual calculated values will depend on the dataset, train/test split, and exact features used. The provided CSV is synthetic and these numbers are achievable with it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importância das Features (Exemplo do Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after one-hot encoding\n",
    "feature_names_out = rf_pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "importances = rf_pipeline.named_steps['regressor'].feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'feature': feature_names_out, 'importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Feature Importances from Random Forest:\")\n",
    "print(feature_importance_df.head(10))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance_df.head(10), palette='mako') # Top 10\n",
    "plt.title('Top 10 Feature Importances (Random Forest)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTML's feature importance chart shows `study_hours_per_day` as most important, followed by `attendance_percentage`, `mental_health_rating`, etc. The actual results from the model might vary slightly in order but should generally highlight similar features as strong predictors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
